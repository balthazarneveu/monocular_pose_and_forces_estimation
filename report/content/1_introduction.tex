\section{Introduction}
\label{sec:intro}

Humans can easily learn how to perform a given movement or manipulate an object by observing others and attempting to replicate their actions. 
In robotics, this behavioral cloning ability would be usefull to train humanoid robots to perform such tasks. 
However, unlike humans, computers still struggle to understand complex human-object interactions from visual data alone. 
Traditional methods like motion capture offer a solution, but they come with high costs and do not leverage the abundant instructional video 
resources available online.

To address this challenge, \citet{li2019estimating} proposed a novel approach for estimating 3D motion and forces of human-object interactions 
from single RGB videos. 

In this work, we critically reviewed and reimplemented certain aspects of their method. Our initial goal was to assess 
the reproducibility and robustness of the original findings, as well as to explore potential enhancements. We focused on understanding the 
underlying methodology, adapting it to a simplified pipeline. Our code is available on~\href{https://github.com/balthazarneveu/monocular_pose_and_forces_estimation}{GitHub}.


