\section{Appendix}
\subsection{Camera calibration}
\label{app:cam_calib}
Since we're working in a controlled environment (not in the wild, unlike the original paper), we calibrate a single camera once and for all,
\begin{itemize}
    \item using a 7x10 printed checkerboard shot in various orientations
    \item  using the OpenCV implementation of the Zhang's method~\cite{Zhang00calib}.
\end{itemize}

Below, we detail the focal length estimation from camera specifications
and make sure it matches with the calibration estimation.
Xiaomi Mi11 Ultra main camera ($2.8\mu m$ pixel pitch) specifications in photo mode:
\begin{itemize}
    \item 24mm focal length - full frame (24x36mm) equivalent
    \item Sensor size 4000x3000 = 12Mpix
\end{itemize}
We end up with a focal length for the photo mode of $f_{\text{pix}}^{\text{photo}}  = 24mm * 4000px / 36mm = 2666px$.

But since we're using a FullHD video mode with a crop factor of around 15\% on each side,
it is needed to rescale the focal length acordingly $f_{\text{pix}}^{\text{video}} = 2666px * 1.3 * \frac{1920px}{4000px} \approx 1664px$.
Calibration method provides a estimated focal length of $1690px$ which is close enough to the specifications.
We assume the camera to be a pinhole and neglect radial distortion.

\subsection{Inverse kinematics}
\label{app:inverse_kinematics}
The goal of Inverse Kinematics applied to our arm problem is to find the $q$ states so that the elbow ($E$) and wrist ($W$)
reach 3D target positions $P_{E}$ and $P_{W}$.
This iterative process is performed by following these steps:
\begin{itemize}
    \item computing the 3D positions of the elbow $\hat{P_{E}}(q)$ and wrist $\hat{P_{W}}(q)$ from the current joint states $q$ using forward kinematics
    \item computing the Jacobians $J_{E}$ and $J_{W}$ of the arm model at the current joint states $q$.
    \item Compute the 3D error vectors 
        \subitem $\Delta_{E} =P_{E} - \hat{P_{E}}(q)$ 
        \subitem $\Delta_{W} =P_{W} - \hat{P_{W}}(q)$. 
    \item We want the elbow to reach the target so we are willing to move the arm with a configuration state increment 
    $\delta q_{E}$ which statisfies $\min_{\delta q_{E}} \|J_{E}.\delta q_{E} - \Delta_{E}\|_2^2.$
    \item This is achieved by computing the pseudo-inverse $J_{E}^+$ of the Jacobian and computing $\delta q_{E} = J_{E}^+ \Delta_{E}$
    \item We also need to move the wrist in the right direction without undoing the elbow movement. This is performed by making an increment for the wrist in the orthogonal null space of the elbow's Jacobian.
    \subitem . Find $\delta q_{W}$ , $\min_{\delta q_{W}\in Ker(J_{E})} \|J_{W} (\delta q_{E} + \delta q_{W}) - \Delta_{W}\|_2^2$
    \subitem . Forcing a vector to be in $Ker(J_{E})$ is achieved using the projection $P_{E}=I-J_{E}^{+}J_{E}$
    \textit{(eg. $\forall \delta q$, $P_{E}.\delta q$ will not change the elbow position)}
    \subitem . $\delta q_{W} = \delta q_{E} + {(J_{W}P_{E})}^{+} (\Delta_{W} - J_{W} \delta q_{E})$
    \item Finally update the joint states $q$ with the increment $\delta q_{W} \Delta T$ where $\Delta T$ defines the convergence speed.
\end{itemize}

Remark: \textit{Jacobian matrix provides the 3D linear velocity 
(and axis-angle angular velocity) at a given joint location when we apply a variation $\delta q_i$ on a specific component $q_i$ of the configuration state $q$.
Note that although the Jacobian provided in Pinocchio provides linear and angular velocities derivatives, we only use the 3D linear velocity part.}

\subsection{Code description}
\label{app:code}
The code is available at:

~\href{https://github.com/balthazarneveu/monocular_pose_and_forces_estimation}{github.com/balthazarneveu/monocular\_pose\_and\_forces\_estimation}

The code is written in Python and relies on a several external libraries:
\begin{itemize}
    \item Pinocchio for kinematics and dynamics computations aswell as the arm model.
    \item Meshcat for 3D visualization.
    \item OpenCV for the camera calibration and the image processing.
    \item MoviePy wraps video processing.
    \item ~\href{https://developers.google.com/mediapipe}{Google Mediapipe} for 2D and  3D pose estimation.
    \item Scipy for the Levenberg-Marquardt optimization.
    \item ~\href{https://github.com/emmcb/batch-processing}{batch-processing} to process multiple video files in a systematic way.
    \item ~\href{https://github.com/balthazarneveu/interactive_pipe}{interactive-pipe} to display a GUI with graphs and images and interact with sliders and keyboard.
    This library works with Matplotlib as the default graphical backend but PyQT/PySide is highly recommended for the demo.
\end{itemize}

To process a new set of videos, located in the \texttt{data} folder, run the following command:
\begin{verbatim}
    python scripts/batch_video_processing.py
    -i "data/*.mp4"
    -o "out"
    -A demo
\end{verbatim}
\textit{When the GUI pops up, press F1 to get the help menu to learn about the shortcuts. Press F11 to display in full screen.
Do not forget to click the hyperlink in the terminal to open the MeshCat viewer in your browser.}


If you're using a different camera, you'll need to calibrate your camera intrinsics first.
Capture a calibration video sequence using a 10x7 checkerboard (print it and stick it on a cardboard or display it on your screen).

\begin{verbatim}
    python scripts/batch_video_processing.py
    -i "data/camera_calibration_<cam_id>.mp4"
    -o "calibration"
    -A camera_calibration
\end{verbatim}

Then you'll be able to process your pose videos using the new camera intrinsics, by simply specifiying
the calibration file path:
\begin{verbatim}
    -calib "calibration/camera_calibration_<cam_id>.yaml"
\end{verbatim}

The core of the code for inverse kinematics and inverse dynamics is located in:
~\href{https://github.com/balthazarneveu/monocular_pose_and_forces_estimation/tree/main/src/projectyl/dynamics}{src/projectyl/dynamics}